{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "setup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8e7a6b9-73ae-4a07-f01e-2c8f37e95152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nctirs-platform-v2'...\n",
            "remote: Enumerating objects: 8256, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 8256 (delta 52), reused 42 (delta 41), pack-reused 8191 (from 2)\u001b[K\n",
            "Receiving objects: 100% (8256/8256), 44.31 MiB | 13.09 MiB/s, done.\n",
            "Resolving deltas: 100% (1604/1604), done.\n",
            "/content/nctirs-platform-v2/ai_training/nctirs-platform-v2/ai_training\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (2.10.0+cpu)\n",
            "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (3.10.0)\n",
            "Requirement already satisfied: seaborn>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (0.13.2)\n",
            "Requirement already satisfied: tensorflow>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (2.19.0)\n",
            "Requirement already satisfied: tensorflowjs>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 16)) (4.22.0)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 19)) (4.67.3)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 20)) (6.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (3.24.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (2025.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2025.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 8)) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 8)) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 8)) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 11)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 11)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 11)) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 11)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 11)) (23.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 11)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 11)) (3.3.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->-r requirements.txt (line 15)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->-r requirements.txt (line 15)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->-r requirements.txt (line 15)) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->-r requirements.txt (line 15)) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->-r requirements.txt (line 15)) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->-r requirements.txt (line 15)) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->-r requirements.txt (line 15)) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->-r requirements.txt (line 15)) (5.29.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->-r requirements.txt (line 15)) (2.32.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->-r requirements.txt (line 15)) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->-r requirements.txt (line 15)) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->-r requirements.txt (line 15)) (2.1.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->-r requirements.txt (line 15)) (1.78.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->-r requirements.txt (line 15)) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->-r requirements.txt (line 15)) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->-r requirements.txt (line 15)) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->-r requirements.txt (line 15)) (0.5.4)\n",
            "Requirement already satisfied: flax>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (0.11.2)\n",
            "Requirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (6.5.2)\n",
            "Requirement already satisfied: jax>=0.4.13 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (0.7.2)\n",
            "Requirement already satisfied: jaxlib>=0.4.13 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (0.7.2)\n",
            "Requirement already satisfied: tf-keras>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (2.19.0)\n",
            "Requirement already satisfied: tensorflow-decision-forests>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (1.12.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.16.1 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (0.16.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow>=2.13.0->-r requirements.txt (line 15)) (0.45.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (1.1.2)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (0.2.7)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (0.11.33)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (0.1.81)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (13.9.4)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (0.1.10)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.13.0->-r requirements.txt (line 15)) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.13.0->-r requirements.txt (line 15)) (0.19.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.13.0->-r requirements.txt (line 15)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.13.0->-r requirements.txt (line 15)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.13.0->-r requirements.txt (line 15)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.13.0->-r requirements.txt (line 15)) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.13.0->-r requirements.txt (line 15)) (3.10.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.13.0->-r requirements.txt (line 15)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.13.0->-r requirements.txt (line 15)) (3.1.6)\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.12/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (3.1.1)\n",
            "Requirement already satisfied: ydf>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (0.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->-r requirements.txt (line 5)) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (2.19.2)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (1.13.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (24.1.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (4.15.0)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (3.20.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (5.9.5)\n",
            "Requirement already satisfied: uvloop in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (0.22.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (0.1.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs>=4.0.0->-r requirements.txt (line 16)) (3.23.0)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "!git clone https://github.com/arapgechina24-lgtm/nctirs-platform-v2.git\n",
        "%cd nctirs-platform-v2/ai_training\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# NCTIRS Platform V2 - AI Training Pipeline\n",
        "This notebook automatically clones your repository, installs dependencies, and runs the entire training and export pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "data_prep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a535ee-2fbd-40d7-85d0-51ed461ff7b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "############################################################\n",
            "# DATASET: CICIDS2017 (Canadian Institute for Cybersecurity)\n",
            "# 8 attack types, 2.8M+ flows, CSE/UNB collaboration\n",
            "############################################################\n",
            "\n",
            "============================================================\n",
            "Downloading: cicids2017_MachineLearningCSV.zip\n",
            "Destination: /content/nctirs-platform-v2/ai_training/data/cicids2017/cicids2017_MachineLearningCSV.zip\n",
            "============================================================\n",
            "  File already exists: /content/nctirs-platform-v2/ai_training/data/cicids2017/cicids2017_MachineLearningCSV.zip (0.1 MB)\n",
            "\n",
            "  Extracting cicids2017_MachineLearningCSV.zip...\n",
            "  ✗ Extraction failed: File is not a zip file\n",
            "\n",
            "############################################################\n",
            "# DATASET: UNSW-NB15 (University of New South Wales)\n",
            "# 10 attack categories, 2.5M+ records, ACCS research\n",
            "############################################################\n",
            "\n",
            "============================================================\n",
            "Downloading: UNSW_NB15_training-set.csv\n",
            "Destination: /content/nctirs-platform-v2/ai_training/data/unsw_nb15/UNSW_NB15_training-set.csv\n",
            "============================================================\n",
            "\n",
            "  ✗ Download failed: HTTP Error 404: Not Found\n",
            "  Trying fallback: https://cloudstor.aarnet.edu.au/plus/index.php/s/2DhnLGDdEECo4ys/download\n",
            "\n",
            "============================================================\n",
            "Downloading: UNSW_NB15_training-set.csv (fallback)\n",
            "Destination: /content/nctirs-platform-v2/ai_training/data/unsw_nb15/UNSW_NB15_training-set.csv\n",
            "============================================================\n",
            "\n",
            "  ✗ Download failed: <urlopen error [Errno -5] No address associated with hostname>\n",
            "\n",
            "============================================================\n",
            "Downloading: UNSW_NB15_testing-set.csv\n",
            "Destination: /content/nctirs-platform-v2/ai_training/data/unsw_nb15/UNSW_NB15_testing-set.csv\n",
            "============================================================\n",
            "\n",
            "  ✗ Download failed: HTTP Error 404: Not Found\n",
            "  Trying fallback: https://cloudstor.aarnet.edu.au/plus/index.php/s/2DhnLGDdEECo4ys/download\n",
            "\n",
            "============================================================\n",
            "Downloading: UNSW_NB15_testing-set.csv (fallback)\n",
            "Destination: /content/nctirs-platform-v2/ai_training/data/unsw_nb15/UNSW_NB15_testing-set.csv\n",
            "============================================================\n",
            "\n",
            "  ✗ Download failed: <urlopen error [Errno -5] No address associated with hostname>\n",
            "\n",
            "============================================================\n",
            "DATASET INVENTORY\n",
            "============================================================\n",
            "\n",
            "  CICIDS2017 (Canadian Institute for Cybersecurity):\n",
            "    Directory: /content/nctirs-platform-v2/ai_training/data/cicids2017\n",
            "    CSV Files: 0\n",
            "    Total Size: 0.0 MB\n",
            "\n",
            "  UNSW-NB15 (University of New South Wales):\n",
            "    Directory: /content/nctirs-platform-v2/ai_training/data/unsw_nb15\n",
            "    CSV Files: 0\n",
            "    Total Size: 0.0 MB\n",
            "\n",
            "  Synthetic Dataset:\n",
            "    File: /content/nctirs-platform-v2/ai_training/data/synthetic_dataset.csv\n",
            "    Size: 80.9 MB\n",
            "\n",
            "————————————————————————————————————————————————————————————\n",
            "  Total: 1 files, 80.9 MB\n",
            "============================================================\n",
            "Generating 80000 normal samples...\n",
            "Generating 2000 DDoS samples...\n",
            "Generating 2000 PortScan samples...\n",
            "Generating 2000 BruteForce samples...\n",
            "Generating 2000 Infiltration samples...\n",
            "Generating 2000 Botnet samples...\n",
            "Generating 2000 WebAttack samples...\n",
            "Generating 2000 Heartbleed samples...\n",
            "Generating 2000 DataExfil samples...\n",
            "Generating 2000 C2Beacon samples...\n",
            "Generating 2000 DNSTunnel samples...\n",
            "\n",
            "============================================================\n",
            "Dataset saved to: /content/nctirs-platform-v2/ai_training/data/synthetic_dataset.csv\n",
            "Total samples: 100000\n",
            "Normal: 80000 (80.0%)\n",
            "Attack: 20000 (20.0%)\n",
            "\n",
            "Attack type distribution:\n",
            "  Botnet: 2000\n",
            "  WebAttack: 2000\n",
            "  PortScan: 2000\n",
            "  Infiltration: 2000\n",
            "  DNSTunnel: 2000\n",
            "  Heartbleed: 2000\n",
            "  BruteForce: 2000\n",
            "  DataExfil: 2000\n",
            "  C2Beacon: 2000\n",
            "  DDoS: 2000\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# 1. Download dataset\n",
        "!python download_datasets.py\n",
        "\n",
        "# 2. Generate synthetic data if needed\n",
        "!python generate_synthetic.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "training",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a007d592-308f-40fe-983a-93818c3ddf74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✗ UNSW-NB15 not found at /content/nctirs-platform-v2/ai_training/data/unsw_nb15/unsw_nb15_full.csv\n",
            "  ✗ CICIDS2017 not found at /content/nctirs-platform-v2/ai_training/data/cicids2017/cicids2017_full.csv\n",
            "\n",
            "[3/3] Loading Synthetic dataset...\n",
            "  ✓ Synthetic: 100000 samples (80000 normal, 20000 attack)\n",
            "  Attack types: {'Botnet': 2000, 'WebAttack': 2000, 'PortScan': 2000, 'Infiltration': 2000, 'DNSTunnel': 2000, 'Heartbleed': 2000, 'BruteForce': 2000, 'DataExfil': 2000, 'C2Beacon': 2000, 'DDoS': 2000}\n",
            "\n",
            "============================================================\n",
            "COMBINED DATASET: 100000 total samples\n",
            "  Normal: 80000 (80.0%)\n",
            "  Attack: 20000 (20.0%)\n",
            "  Features: 46\n",
            "============================================================\n",
            "\n",
            "Train: 69991 | Val: 14991 | Test: 14991\n",
            "Model: 71,472 parameters | Device: cpu\n",
            "\n",
            "============================================================\n",
            "TRAINING — 100 epochs, batch 256, LR 0.001\n",
            "============================================================\n",
            "\n",
            "Epoch    1/100 | Train: 0.6824 acc=0.8991 | Val: 0.6082 acc=0.9984 | LR: 1.00e-03 | 24s\n",
            "Epoch    5/100 | Train: 0.5705 acc=0.9987 | Val: 0.5559 acc=0.9993 | LR: 1.00e-03 | 119s\n",
            "Epoch   10/100 | Train: 0.5493 acc=0.9993 | Val: 0.5386 acc=0.9994 | LR: 1.00e-03 | 236s\n",
            "Epoch   15/100 | Train: 0.5290 acc=0.9996 | Val: 0.5093 acc=0.9995 | LR: 1.00e-03 | 354s\n",
            "Epoch   20/100 | Train: 0.5147 acc=0.9996 | Val: 0.5020 acc=0.9996 | LR: 1.00e-03 | 472s\n",
            "Epoch   25/100 | Train: 0.5104 acc=0.9996 | Val: 0.4982 acc=0.9997 | LR: 1.00e-03 | 590s\n",
            "Epoch   30/100 | Train: 0.5076 acc=0.9998 | Val: 0.4947 acc=0.9995 | LR: 1.00e-03 | 708s\n",
            "Epoch   35/100 | Train: 0.5044 acc=0.9997 | Val: 0.4921 acc=0.9996 | LR: 1.00e-03 | 825s\n",
            "Epoch   40/100 | Train: 0.5019 acc=0.9997 | Val: 0.4885 acc=0.9997 | LR: 1.00e-03 | 942s\n",
            "Epoch   45/100 | Train: 0.4990 acc=0.9998 | Val: 0.4865 acc=0.9996 | LR: 1.00e-03 | 1059s\n",
            "Epoch   50/100 | Train: 0.4970 acc=0.9999 | Val: 0.4848 acc=0.9995 | LR: 1.00e-03 | 1176s\n",
            "Epoch   55/100 | Train: 0.4952 acc=0.9998 | Val: 0.4837 acc=0.9996 | LR: 1.00e-03 | 1293s\n",
            "Epoch   60/100 | Train: 0.4933 acc=0.9998 | Val: 0.4820 acc=0.9994 | LR: 1.00e-03 | 1410s\n",
            "Epoch   65/100 | Train: 0.4915 acc=0.9998 | Val: 0.4804 acc=0.9996 | LR: 1.00e-03 | 1527s\n",
            "Epoch   70/100 | Train: 0.4895 acc=0.9999 | Val: 0.4796 acc=0.9997 | LR: 1.00e-03 | 1644s\n",
            "Epoch   75/100 | Train: 0.4885 acc=0.9998 | Val: 0.4776 acc=0.9996 | LR: 1.00e-03 | 1761s\n",
            "Epoch   80/100 | Train: 0.4870 acc=0.9999 | Val: 0.4770 acc=0.9997 | LR: 1.00e-03 | 1877s\n",
            "Epoch   85/100 | Train: 0.4861 acc=0.9998 | Val: 0.4763 acc=0.9996 | LR: 1.00e-03 | 1995s\n",
            "Epoch   90/100 | Train: 0.4851 acc=0.9999 | Val: 0.4750 acc=0.9997 | LR: 1.00e-03 | 2112s\n",
            "Epoch   95/100 | Train: 0.4840 acc=0.9999 | Val: 0.4746 acc=0.9996 | LR: 1.00e-03 | 2229s\n",
            "Epoch  100/100 | Train: 0.4832 acc=0.9998 | Val: 0.4733 acc=0.9996 | LR: 1.00e-03 | 2346s\n",
            "\n",
            "============================================================\n",
            "Training complete in 2345.8s\n",
            "Best epoch: 99 | Best val loss: 0.4727\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EVALUATING ON TEST SET (14991 samples)\n",
            "============================================================\n",
            "\n",
            "--- CLASSIFICATION HEAD ---\n",
            "  Accuracy:   0.9998\n",
            "  Precision:  0.9990\n",
            "  Recall:     1.0000\n",
            "  F1 (macro): 0.9997\n",
            "  F1 (binary):0.9995\n",
            "  ROC AUC:    1.0000\n",
            "  Confusion:  [[11991, 3], [0, 2997]]\n",
            "\n",
            "--- RECONSTRUCTION ---\n",
            "  Threshold:  0.8532\n",
            "  Accuracy:   0.7865\n",
            "  F1 (macro): 0.5378\n",
            "\n",
            "============================================================\n",
            "EXPORTING MODEL FOR BROWSER INFERENCE\n",
            "============================================================\n",
            "  Weights: /content/nctirs-platform-v2/public/models/anomaly-detector-v2/weights.bin (282.2 KB)\n",
            "\n",
            "  Model metadata: /content/nctirs-platform-v2/public/models/anomaly-detector-v2/model_metadata.json\n",
            "  Normalization:  /content/nctirs-platform-v2/public/models/anomaly-detector-v2/normalization_stats.json\n",
            "\n",
            "============================================================\n",
            "EXPORT COMPLETE — All files in /content/nctirs-platform-v2/public/models/anomaly-detector-v2\n",
            "  evaluation_report.json (2.1 KB)\n",
            "  model.json (8.8 KB)\n",
            "  model_metadata.json (3.6 KB)\n",
            "  normalization_stats.json (8.4 KB)\n",
            "  weights.bin (282.2 KB)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# 3. Train the model (Using the combined pipeline)\n",
        "!python train_combined.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "evaluation",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abde7aa0-212c-43f9-aca9-07ec6525682a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkpoint from epoch 3 (val_loss: 0.2340)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nctirs-platform-v2/ai_training/nctirs-platform-v2/ai_training/evaluate.py\", line 227, in <module>\n",
            "    main()\n",
            "  File \"/content/nctirs-platform-v2/ai_training/nctirs-platform-v2/ai_training/evaluate.py\", line 181, in main\n",
            "    features, labels = loader.load_synthetic()\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nctirs-platform-v2/ai_training/nctirs-platform-v2/ai_training/dataset_loader.py\", line 145, in load_synthetic\n",
            "    df = pd.read_csv(path)\n",
            "         ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n",
            "    return _read(filepath_or_buffer, kwds)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 620, in _read\n",
            "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n",
            "    self._engine = self._make_engine(f, self.engine)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 1880, in _make_engine\n",
            "    self.handles = get_handle(\n",
            "                   ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\", line 873, in get_handle\n",
            "    handle = open(\n",
            "             ^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/nctirs-platform-v2/ai_training/nctirs-platform-v2/ai_training/data/synthetic_dataset.csv'\n"
          ]
        }
      ],
      "source": [
        "# 4. Evaluate the trained models\n",
        "!python evaluate.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "export",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "228b3552-4049-4589-922f-9f0a63e6e288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkpoint: epoch 3, val_loss 0.2340\n",
            "\n",
            "============================================================\n",
            "Exporting Model: SENTINEL-OMEGA-ADv2 v2.0.0\n",
            "Output: /content/nctirs-platform-v2/ai_training/nctirs-platform-v2/public/models/anomaly-detector-v2\n",
            "============================================================\n",
            "\n",
            "Model exported to /content/nctirs-platform-v2/ai_training/nctirs-platform-v2/public/models/anomaly-detector-v2/model.json\n",
            "Weights saved to /content/nctirs-platform-v2/ai_training/nctirs-platform-v2/public/models/anomaly-detector-v2/weights.bin (282.2 KB)\n",
            "Normalization stats exported to /content/nctirs-platform-v2/ai_training/nctirs-platform-v2/public/models/anomaly-detector-v2/normalization_stats.json\n",
            "Model metadata exported to /content/nctirs-platform-v2/ai_training/nctirs-platform-v2/public/models/anomaly-detector-v2/model_metadata.json\n",
            "Evaluation report copied to /content/nctirs-platform-v2/ai_training/nctirs-platform-v2/public/models/anomaly-detector-v2/evaluation_report.json\n",
            "\n",
            "============================================================\n",
            "Export complete! Files in /content/nctirs-platform-v2/ai_training/nctirs-platform-v2/public/models/anomaly-detector-v2:\n",
            "  evaluation_report.json (0.5 KB)\n",
            "  model.json (8.8 KB)\n",
            "  model_metadata.json (3.1 KB)\n",
            "  normalization_stats.json (7.9 KB)\n",
            "  weights.bin (282.2 KB)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# 5. Export weights for use in the web app\n",
        "!python export_weights.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "download_artifacts",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "cd9d71ad-12b6-4bb9-9031-ca8d35ce2b39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zipping checkpoints...\n",
            "  adding: checkpoints/ (stored 0%)\n",
            "  adding: checkpoints/best_model.pt (deflated 10%)\n",
            "Downloading checkpoints...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b724f1fa-df89-498f-8b72-eb130f635526\", \"model_checkpoints.zip\", 821040)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 6. Download the generated weights to your local machine\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"Zipping checkpoints...\")\n",
        "!zip -r model_checkpoints.zip checkpoints/\n",
        "\n",
        "if os.path.exists('model_checkpoints.zip'):\n",
        "    print(\"Downloading checkpoints...\")\n",
        "    files.download('model_checkpoints.zip')\n",
        "else:\n",
        "    print(\"Could not find model_checkpoints.zip\")"
      ]
    }
  ]
}